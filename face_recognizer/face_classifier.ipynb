{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d201f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fbf2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mtcnn = MTCNN(image_size=240, margin=0, min_face_size=20) # initializing mtcnn for face detection\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() # initializing resnet for face img to embeding conversion\n",
    "\n",
    "dataset=datasets.ImageFolder('photos') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "face_list = [] # list of cropped faces from photos folder\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn(img, return_prob=True) \n",
    "    if face is not None and prob>0.90: # if face detected and porbability > 90%\n",
    "        emb = resnet(face.unsqueeze(0)) # passing cropped face into resnet model to get embedding matrix\n",
    "        embedding_list.append(emb.detach()) # resulten embedding matrix is stored in a list\n",
    "        name_list.append(idx_to_class[idx]) # names are stored in a list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07291146",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [embedding_list, name_list]\n",
    "torch.save(data, 'data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4914fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[251 248 194 194]]\n",
      "[[233 218 232 232]]\n",
      "[[234 207 233 233]]\n",
      "[[236 195 232 232]]\n",
      "[[238 185 234 234]]\n",
      "[[241 176 245 245]]\n",
      "[[247 166 254 254]]\n",
      "[[256 159 258 258]]\n",
      "[[261 147 263 263]]\n",
      "[[255 114 283 283]]\n",
      "[[252  90 278 278]]\n",
      "[[248  79 275 275]]\n",
      "[[237  69 272 272]]\n",
      "[[227  83 266 266]]\n",
      "[[229 142 258 258]]\n",
      "[[232 167 253 253]]\n",
      "[[231 172 250 250]]\n",
      "[[233 177 243 243]]\n",
      "[[238 184 244 244]]\n",
      "[[248 195 236 236]]\n",
      "[[249 198 237 237]]\n",
      "[[257 201 226 226]]\n",
      "[[260 201 223 223]]\n",
      "[[264 202 214 214]]\n",
      "[[262 198 218 218]]\n",
      "user1.jpg\n",
      "[[261 202 213 213]]\n",
      "[[252 200 223 223]]\n",
      "[[248 202 219 219]]\n",
      "[[242 202 218 218]]\n",
      "[[239 200 225 225]]\n",
      "[[235 205 216 216]]\n",
      "[[233 206 216 216]]\n",
      "[[235 207 211 211]]\n",
      "[[231 207 213 213]]\n",
      "[[230 207 212 212]]\n",
      "[[229 205 215 215]]\n",
      "[[232 207 211 211]]\n",
      "[[230 203 216 216]]\n",
      "[[230 204 212 212]]\n",
      "[[229 204 214 214]]\n",
      "[[230 204 215 215]]\n",
      "[[233 205 212 212]]\n",
      "[[234 206 210 210]]\n",
      "[[234 206 210 210]]\n",
      "[[235 206 209 209]]\n",
      "[[235 205 210 210]]\n",
      "[[237 206 209 209]]\n",
      "[[237 206 209 209]]\n",
      "[[234 205 211 211]]\n",
      "[[234 205 212 212]]\n",
      "user2.jpg\n",
      "[[232 203 212 212]]\n",
      "[[226 204 212 212]]\n",
      "[[225 205 209 209]]\n",
      "[[222 205 210 210]]\n",
      "[[219 206 206 206]]\n",
      "[[218 204 209 209]]\n",
      "[[219 207 205 205]]\n",
      "[[219 205 206 206]]\n",
      "[[219 205 210 210]]\n",
      "[[221 203 213 213]]\n",
      "[[221 203 213 213]]\n",
      "[[226 206 212 212]]\n",
      "[[233 207 210 210]]\n",
      "[[238 205 216 216]]\n",
      "[[240 203 218 218]]\n",
      "[[243 202 219 219]]\n",
      "[[242 202 218 218]]\n",
      "[[240 199 220 220]]\n",
      "[[238 201 218 218]]\n",
      "[[236 204 211 211]]\n",
      "[[236 204 211 211]]\n",
      "[[227 204 208 208]]\n",
      "[[214 204 207 207]]\n",
      "[[214 204 207 207]]\n",
      "[[202 206 202 202]]\n",
      "user3.jpg\n",
      "[[186 206 206 206]]\n",
      "[[186 206 206 206]]\n",
      "[[183 208 200 200]]\n",
      "[[184 209 204 204]]\n",
      "[[184 209 204 204]]\n",
      "[[186 204 209 209]]\n",
      "[[194 208 205 205]]\n",
      "[[194 208 205 205]]\n",
      "[[198 205 208 208]]\n",
      "[[201 206 208 208]]\n",
      "[[204 206 209 209]]\n",
      "[[205 207 210 210]]\n",
      "[[205 207 210 210]]\n",
      "[[204 205 213 213]]\n",
      "[[204 205 212 212]]\n",
      "[[204 205 212 212]]\n",
      "[[206 206 208 208]]\n",
      "[[203 203 212 212]]\n",
      "[[203 203 212 212]]\n",
      "[[205 206 210 210]]\n",
      "[[206 209 205 205]]\n",
      "[[206 209 205 205]]\n",
      "[[205 208 208 208]]\n",
      "[[204 207 211 211]]\n",
      "[[204 207 211 211]]\n",
      "user4.jpg\n",
      "[[205 208 211 211]]\n",
      "[[204 209 211 211]]\n",
      "[[204 209 211 211]]\n",
      "[[202 206 212 212]]\n",
      "[[206 208 208 208]]\n",
      "[[206 208 208 208]]\n",
      "[[205 208 209 209]]\n",
      "[[202 206 213 213]]\n",
      "[[202 206 213 213]]\n",
      "[[204 205 213 213]]\n",
      "[[205 205 210 210]]\n",
      "[[205 205 210 210]]\n",
      "[[205 205 209 209]]\n",
      "[[206 206 208 208]]\n",
      "[[206 206 208 208]]\n",
      "[[204 204 212 212]]\n",
      "[[210 208 203 203]]\n",
      "[[210 208 203 203]]\n",
      "[[209 206 204 204]]\n",
      "[[208 206 206 206]]\n",
      "[[208 206 206 206]]\n",
      "[[207 205 207 207]]\n",
      "[[208 206 205 205]]\n",
      "[[208 206 204 204]]\n",
      "[[208 206 204 204]]\n",
      "user5.jpg\n",
      "Image didn't match\n",
      "Face matched with:  kashif With distance:  0.8774987816810608\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def draw_rect(img, faces, sf=20):\n",
    "    for x, y, w, h in faces:\n",
    "        x = x - sf\n",
    "        y = y - 2*sf\n",
    "        cv2.rectangle(img, (x,y), (x+w+2*sf, y+h+3*sf), (255, 0 , 0), 3)\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "frameFrequency=25\n",
    "total_frame = 0\n",
    "id = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read(0)\n",
    "\n",
    "    if ret is False:\n",
    "        break\n",
    "\n",
    "    faces = face_detector.detectMultiScale(frame, 1.1, 4)\n",
    "    print(faces)\n",
    "\n",
    "    draw_rect(frame, faces)\n",
    "    \n",
    "    total_frame += 1\n",
    "    if total_frame%frameFrequency == 0:\n",
    "        id += 1\n",
    "        image_name = \"user\" + str(id) +'.jpg'\n",
    "        cv2.imwrite(image_name, frame)\n",
    "        print(image_name)\n",
    "    # Display the output\n",
    "    cv2.imshow('Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    if id >=5:        \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "def face_match(img_path, data_path): # img_path= location of photo, data_path= location of data.pt \n",
    "    # getting embedding matrix of the given img\n",
    "    img = Image.open(img_path)\n",
    "    face, prob = mtcnn(img, return_prob=True) # returns cropped face and probability\n",
    "    emb = resnet(face.unsqueeze(0)).detach() # detech is to make required gradient false\n",
    "    \n",
    "    saved_data = torch.load('data.pt') # loading data.pt file\n",
    "    embedding_list = saved_data[0] # getting embedding data\n",
    "    name_list = saved_data[1] # getting list of names\n",
    "    dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "    \n",
    "    for idx, emb_db in enumerate(embedding_list):\n",
    "        dist = torch.dist(emb, emb_db).item()\n",
    "        dist_list.append(dist)\n",
    "        \n",
    "    idx_min = dist_list.index(min(dist_list))\n",
    "    return (name_list[idx_min], min(dist_list))\n",
    "\n",
    "\n",
    "\n",
    "z=0\n",
    "total_images_to_be_clicked=5\n",
    "for i in range(1,6):\n",
    "    name = \"user\" +str(i) +\".jpg\"\n",
    "# name=\"user3.jpg\"\n",
    "    result = face_match(name, 'data.pt')\n",
    "    \n",
    "    z = z+ result[1]\n",
    "    \n",
    "z=z/total_images_to_be_clicked\n",
    "if(z>0.8):\n",
    "    print(\"Image didn't match\")\n",
    "elif(z<=0.8):\n",
    "    print(\"Image matched with existing user\")\n",
    "\n",
    "print('Face matched with: ',result[0], 'With distance: ',z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa247f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
